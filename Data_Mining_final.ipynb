{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Culty48/MillionSongDatasetMining/blob/main/Data_Mining_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUIvYxBAVOTj"
      },
      "outputs": [],
      "source": [
        "# ---------------- RUN ONCE: SAVING DATA ON THE DRIVE----------------\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1️ Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2️ Paths in Drive\n",
        "dataset_path = '/content/drive/MyDrive/MillionSongSubset'\n",
        "getters_path = '/content/drive/MyDrive/hdf5_getters.py'\n",
        "\n",
        "# 3️ Download & extract Million Song Subset if not already in Drive\n",
        "if not os.path.exists(dataset_path):\n",
        "    print(\"Downloading Million Song Subset...\")\n",
        "    !wget -O MillionSongSubset.tar.gz http://labrosa.ee.columbia.edu/~dpwe/tmp/millionsongsubset.tar.gz\n",
        "    !mkdir -p \"$dataset_path\"\n",
        "    !tar -xvzf MillionSongSubset.tar.gz -C \"$dataset_path\"\n",
        "    !rm MillionSongSubset.tar.gz\n",
        "    print(\" Dataset downloaded and extracted to Drive.\")\n",
        "else:\n",
        "    print(\"Million Song Subset already exists in Drive.\")\n",
        "\n",
        "# 4️ Download hdf5_getters.py if not already in Drive\n",
        "if not os.path.exists(getters_path):\n",
        "    print(\"Downloading hdf5_getters.py...\")\n",
        "    !wget -O \"$getters_path\" https://raw.githubusercontent.com/tbertinmahieux/MSongsDB/master/PythonSrc/hdf5_getters.py\n",
        "    print(\" hdf5_getters.py downloaded to Drive.\")\n",
        "else:\n",
        "    print(\"hdf5_getters.py already exists in Drive.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "2JYuUTksVdrQ",
        "outputId": "5e525db4-4134-46cb-bd84-c21cd8770a3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Patched hdf5_getters.py for PyTables compatibility\n",
            "hdf5_getters imported successfully!\n",
            "Number of songs found: 10000\n",
            "First 5 files: ['/content/drive/MyDrive/MillionSongSubset/MillionSongSubset/A/M/G/TRAMGBE128F148B476.h5', '/content/drive/MyDrive/MillionSongSubset/MillionSongSubset/A/M/G/TRAMGUT12903CEDE40.h5', '/content/drive/MyDrive/MillionSongSubset/MillionSongSubset/A/M/G/TRAMGTB128F4251586.h5', '/content/drive/MyDrive/MillionSongSubset/MillionSongSubset/A/M/G/TRAMGZG12903CC672C.h5', '/content/drive/MyDrive/MillionSongSubset/MillionSongSubset/A/M/G/TRAMGZF128F4292DA9.h5']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2645231285.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0martist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_getters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_artist_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhdf5_getters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mh5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" {artist} – {title}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tables/file.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2924\u001b[0m         \u001b[0;31m# Close the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2925\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2927\u001b[0m         \u001b[0;31m# After the objects are disconnected, destroy the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# --- FIX THE GETTER ISSUE ---\n",
        "!sed -i 's/tables.openFile/tables.open_file/g' /content/drive/MyDrive/hdf5_getters.py\n",
        "print(\" Patched hdf5_getters.py for PyTables compatibility\")\n",
        "\n",
        "# ---------------- REUSABLE ----------------\n",
        "import sys\n",
        "import glob\n",
        "import importlib\n",
        "\n",
        "# 1️ Add Drive folder to Python path\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "\n",
        "# 2️ Import or reload hdf5_getters\n",
        "import hdf5_getters\n",
        "importlib.reload(hdf5_getters)\n",
        "print(\"hdf5_getters imported successfully!\")\n",
        "\n",
        "# 3️ Path to dataset\n",
        "dataset_path = '/content/drive/MyDrive/MillionSongSubset'\n",
        "\n",
        "# 4️ Find all .h5 files recursively\n",
        "files = glob.glob(dataset_path + '/**/*.h5', recursive=True)\n",
        "print(f\"Number of songs found: {len(files)}\")\n",
        "print(\"First 5 files:\", files[:5])\n",
        "\n",
        "# 5️ Open the first song and read metadata\n",
        "if files:\n",
        "    h5 = hdf5_getters.open_h5_file_read(files[1])\n",
        "    artist = hdf5_getters.get_artist_name(h5).decode('utf-8')\n",
        "    title = hdf5_getters.get_title(h5).decode('utf-8')\n",
        "    h5.close()\n",
        "    print(f\" {artist} – {title}\")\n",
        "else:\n",
        "    print(\"No .h5 files found in dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UtL96KnVhr1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import h5py\n",
        "import sys\n",
        "\n",
        "# Path to the Million Song Subset data\n",
        "dataset_path = '/content/drive/MyDrive/MillionSongSubset'\n",
        "\n",
        "# Path to hdf5_getters.py\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "import hdf5_getters as GETTERS\n",
        "\n",
        "\n",
        "def load_mss_dataframe(dataset_path, max_files=None):\n",
        "    \"\"\"\n",
        "    Loads song data from the Million Song Subset into a pandas DataFrame.\n",
        "    max_files: limit number of songs for faster testing (optional)\n",
        "    \"\"\"\n",
        "\n",
        "    rows = []\n",
        "    count = 0\n",
        "\n",
        "    for root, dirs, files in os.walk(dataset_path):\n",
        "\n",
        "        for file in files:\n",
        "            if not file.endswith('.h5'):\n",
        "                continue\n",
        "\n",
        "            file_path = os.path.join(root, file)\n",
        "\n",
        "            try:\n",
        "              h5 = GETTERS.open_h5_file_read(file_path)\n",
        "            except:\n",
        "              print(\"Skipping unreadable file:\", file_path)\n",
        "              continue\n",
        "\n",
        "            # ---- Extract attributes----\n",
        "            try:\n",
        "                row = {\n",
        "                    \"song_hotttnesss\":       GETTERS.get_song_hotttnesss(h5),\n",
        "                    \"artist_hotttnesss\":     GETTERS.get_artist_hotttnesss(h5),\n",
        "                    \"artist_familiarity\":    GETTERS.get_artist_familiarity(h5),\n",
        "                    \"year\":                  GETTERS.get_year(h5),\n",
        "                    \"duration\":              GETTERS.get_duration(h5),\n",
        "                    \"energy\":                GETTERS.get_energy(h5),\n",
        "                    \"tempo\":                 GETTERS.get_tempo(h5),\n",
        "                    \"loudness\":              GETTERS.get_loudness(h5),\n",
        "                    \"mode\":                  GETTERS.get_mode(h5),\n",
        "                    \"key\":                   GETTERS.get_key(h5),\n",
        "                    \"time_signature\":        GETTERS.get_time_signature(h5),\n",
        "                    \"title\":                 GETTERS.get_title(h5).decode('utf-8', errors='ignore'),\n",
        "                    \"artist_name\":           GETTERS.get_artist_name(h5).decode('utf-8', errors='ignore'),\n",
        "                    \"track_id\":              GETTERS.get_track_id(h5).decode('utf-8', errors='ignore'),\n",
        "                }\n",
        "            except:\n",
        "                h5.close()\n",
        "                continue\n",
        "\n",
        "            rows.append(row)\n",
        "            h5.close()\n",
        "\n",
        "            count += 1\n",
        "            if max_files and count >= max_files:\n",
        "                break\n",
        "\n",
        "        if max_files and count >= max_files:\n",
        "            break\n",
        "\n",
        "    return pd.DataFrame(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mMFi0u5Vl60"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "\n",
        "def preprocess_data(train_df, test_df):\n",
        "    \"\"\"\n",
        "    Preprocess the Million Song Subset dataset.\n",
        "    Parameters: df dataset to be preprocessed\n",
        "    Returns: X_train_selected, X_test_selected, y_train, y_test\n",
        "    These outputs are fully ready to be passed into the custom KNN algorithm.\n",
        "    \"\"\"\n",
        "    # Remove songs with missing hotttness values\n",
        "    train_df = train_df.dropna(subset=['song_hotttnesss'])\n",
        "    test_df = test_df.dropna(subset=['song_hotttnesss'])\n",
        "\n",
        "    # Extract target variable (continuous value)\n",
        "    y_train = train_df['song_hotttnesss'].values\n",
        "    y_test = test_df['song_hotttnesss'].values\n",
        "\n",
        "    # Drop non-predictive identifiers and keep only features\n",
        "    X_train = train_df.drop(['song_hotttnesss', 'track_id', 'title', 'artist_name'],\n",
        "                axis=1, errors='ignore')\n",
        "    X_test = test_df.drop(['song_hotttnesss', 'track_id', 'title', 'artist_name'],\n",
        "                axis=1, errors='ignore')\n",
        "\n",
        "    # Identify numeric and categorical columns\n",
        "    num_features = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "    cat_features = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "    # Pipeline for numeric columns:\n",
        "    #    - Impute missing values using median\n",
        "    #    - uses median to ignore large and noisy data, handles skewed distributions\n",
        "    #    - Scale using StandardScaler\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Pipeline for categorical columns:\n",
        "    #    - Impute missing categories\n",
        "    #    - Uses most frequent (mode) because arithmetic is not supported\n",
        "    #    - Mode is usually right\n",
        "    #    - Convert categories using One-Hot Encoding\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine numeric + categorical preprocessing\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, num_features),\n",
        "            ('cat', categorical_transformer, cat_features)\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Fit preprocessing on training data, transform both sets\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Feature Selection using RandomForestRegressor:\n",
        "    #     - Learns which features are most important for predicting hotttness\n",
        "    #     - Selects only the informative ones\n",
        "    selector = SelectFromModel(\n",
        "        estimator=RandomForestRegressor(n_estimators=200, random_state=42),\n",
        "        threshold=\"median\"  # keep features >= median importance, median ignores noisy data\n",
        "    )\n",
        "\n",
        "    # Fit selector on processed training data\n",
        "    selector.fit(X_train_processed, y_train)\n",
        "\n",
        "    # Apply feature selection to both train and test sets\n",
        "    X_train_selected = selector.transform(X_train_processed)\n",
        "    X_test_selected = selector.transform(X_test_processed)\n",
        "\n",
        "    return X_train_selected, X_test_selected, y_train, y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR3dFh73ZW_e"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Map regression hotness value to a class (0=low, 1=medium, 2=high)\n",
        "def label_map(h, y_train):\n",
        "    threshold_low = np.percentile(y_train, 33)\n",
        "    threshold_high = np.percentile(y_train, 66)\n",
        "\n",
        "    if h < threshold_low:\n",
        "        return 0\n",
        "    elif h < threshold_high:\n",
        "        return 1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "# K-Fold CV and hyperparameter tuning for custom knn_hybrid\n",
        "# Returns best k and average accuracy per k\n",
        "def cross_val_tune_k(df, k_values, n_splits=5, target_col='song_hotttnesss', id_col='track_id'):\n",
        "    fold_result = []\n",
        "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for train_index, test_index in kf.split(df):\n",
        "        # Split data for training and testing data\n",
        "        train_data = df.iloc[train_index].copy()\n",
        "        test_data = df.iloc[test_index].copy()\n",
        "        test_ids = test_data[id_col].values  # Save test IDs before preprocessing\n",
        "\n",
        "        # Preprocess fold\n",
        "        X_train_sel, X_test_sel, y_train, y_test = preprocess_data(train_data, test_data)\n",
        "\n",
        "        # Get test IDs after dropping missing targets\n",
        "        test_ids = test_data.dropna(subset=[target_col])[id_col].values\n",
        "\n",
        "        # Map true targets to numeric labels using loop\n",
        "        y_true_labels = []\n",
        "        for h in y_test:\n",
        "            y_true_labels.append(label_map(h, y_train))\n",
        "\n",
        "        # Loop over candidate k values\n",
        "        for k in k_values:\n",
        "            # Predict with KNN\n",
        "            result_df = knn_hybrid(X_test_sel, X_train_sel, y_train, test_ids, k)\n",
        "\n",
        "            # Convert predicted class names to numeric values\n",
        "            y_pred_labels = result_df['Hotness CLASS'].map({'low': 0, 'medium': 1, 'high': 2}).values\n",
        "\n",
        "            # Compute accuracy\n",
        "            acc = accuracy_score(y_true_labels, y_pred_labels)\n",
        "            fold_result.append((k, acc))\n",
        "\n",
        "    # Convert results to DataFrame for aggregation\n",
        "    results_df = pd.DataFrame(fold_result, columns=['k', 'accuracy'])\n",
        "\n",
        "    # Compute average accuracy per k and select best k\n",
        "    avg_scores = results_df.groupby('k')['accuracy'].mean()\n",
        "    best_k = avg_scores.idxmax()\n",
        "\n",
        "    return best_k, avg_scores.to_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-6vQYgaXYGg"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def knn_hybrid(X_test, X_train, y_train, test_ids, k):\n",
        "\n",
        "    # Thresholds based on training hotness\n",
        "    threshold_low = np.percentile(y_train, 33)\n",
        "    threshold_high = np.percentile(y_train, 66)\n",
        "\n",
        "    output_list = []\n",
        "\n",
        "    # For each test instance\n",
        "    for i, x in enumerate(X_test):\n",
        "        # Euclidean distances to all train points\n",
        "        distances = np.sqrt(np.sum((X_train - x)**2, axis=1))\n",
        "\n",
        "        # pick k nearest neighbors\n",
        "        neighbors_k = np.argsort(distances)[:k]\n",
        "\n",
        "        # regression: average their hotness\n",
        "        predicted_hotness = np.mean(y_train[neighbors_k])\n",
        "\n",
        "        # classify\n",
        "        label = \"\"\n",
        "        if predicted_hotness < threshold_low:\n",
        "            label = \"low\"\n",
        "        elif predicted_hotness < threshold_high:\n",
        "            label = \"medium\"\n",
        "        else:\n",
        "            label = \"high\"\n",
        "\n",
        "        # store in (ORDER_ID, CLASS) form\n",
        "        output_list.append((test_ids[i], label))\n",
        "\n",
        "    # convert to DataFrame in final required format\n",
        "    result_df = pd.DataFrame(output_list, columns=[\"ORDER_ID\", \"Hotness CLASS\"])\n",
        "\n",
        "    return result_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nx29ElYVqj9"
      },
      "outputs": [],
      "source": [
        "df = load_mss_dataframe(dataset_path, max_files=2500)\n",
        "df.head()\n",
        "\n",
        "k_values = [3,5,7,9,11,13]\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "best_k, avg_scores = cross_val_tune_k(df, k_values, n_splits=5, target_col='song_hotttnesss', id_col='track_id')\n",
        "\n",
        "\n",
        "# Tune k using only training data\n",
        "best_k, avg_scores = cross_val_tune_k(train_df, k_values, n_splits=5)\n",
        "print(\"Best k:\", best_k)\n",
        "print(\"Average accuracy per k:\", avg_scores)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert dictionary to sorted lists\n",
        "ks = sorted(avg_scores.keys())\n",
        "accs = [avg_scores[k] for k in ks]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(ks, accs, marker='o', linestyle='-', linewidth=2)\n",
        "\n",
        "plt.title(\"Average Cross-Validation Accuracy vs. K Value\")\n",
        "plt.xlabel(\"K Value (Number of Neighbors)\")\n",
        "plt.ylabel(\"Average Accuracy\")\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.xticks(ks)\n",
        "plt.ylim(0.4, 0.6)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB__XLM9WJ4O"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting (best model)\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def train_gradient_boosting(X_train, y_train, X_test, threshold_low, threshold_high):\n",
        "    # Train model\n",
        "    gbr = GradientBoostingRegressor(\n",
        "      n_estimators=300,\n",
        "      learning_rate=0.05,\n",
        "      max_depth=3,\n",
        "      random_state=42\n",
        "    )\n",
        "    gbr.fit(X_train, y_train)\n",
        "\n",
        "    # Predict numeric hotness\n",
        "    y_pred_hotness = gbr.predict(X_test)\n",
        "\n",
        "    # Convert to labels\n",
        "    def to_label(h):\n",
        "        if h < threshold_low:\n",
        "            return \"low\"\n",
        "        elif h < threshold_high:\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"high\"\n",
        "\n",
        "    y_pred_label = [to_label(v) for v in y_pred_hotness]\n",
        "\n",
        "    return y_pred_label, y_pred_hotness\n",
        "\n",
        "\n",
        "# Gaussian Naive Bayes (bad model?)\n",
        "def train_gaussian_nb(X_train, y_train, X_test, threshold_low, threshold_high):\n",
        "    \"\"\"\n",
        "    3-class Gaussian Naive Bayes classification.\n",
        "    Converts numeric hotness → categorical labels (low/medium/high),\n",
        "    then trains a GNB classifier directly.\n",
        "    \"\"\"\n",
        "\n",
        "    # ---- Convert numeric hotness → labels for training ----\n",
        "    def to_label(h):\n",
        "        if h < threshold_low:\n",
        "            return \"low\"\n",
        "        elif h < threshold_high:\n",
        "            return \"medium\"\n",
        "        else:\n",
        "            return \"high\"\n",
        "\n",
        "    y_train_label = [to_label(v) for v in y_train]\n",
        "\n",
        "    # ---- Train Gaussian Naive Bayes classifier ----\n",
        "    gnb = GaussianNB()\n",
        "    gnb.fit(X_train, y_train_label)\n",
        "\n",
        "    # ---- Predict labels directly ----\n",
        "    y_pred_label = gnb.predict(X_test)\n",
        "\n",
        "    return y_pred_label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z1l7xlAj2KP"
      },
      "outputs": [],
      "source": [
        "# Compare accuracy\n",
        "\n",
        "# === Preprocess train/test split ===\n",
        "X_train, X_test, y_train, y_test = preprocess_data(train_df, test_df)\n",
        "\n",
        "# Track IDs for later output\n",
        "test_ids = test_df[\"track_id\"].values\n",
        "\n",
        "\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "# Convert y_test numeric → labels\n",
        "def convert_to_labels(y_test, threshold_low, threshold_high):\n",
        "    labels = []\n",
        "    for v in y_test:\n",
        "        if v < threshold_low:\n",
        "            labels.append(\"low\")\n",
        "        elif v < threshold_high:\n",
        "            labels.append(\"medium\")\n",
        "        else:\n",
        "            labels.append(\"high\")\n",
        "    return labels\n",
        "\n",
        "# Threshold from training set\n",
        "threshold_low  = np.percentile(y_train, 33)\n",
        "threshold_high = np.percentile(y_train, 66)\n",
        "\n",
        "y_test_label = convert_to_labels(y_test, threshold_low, threshold_high)\n",
        "\n",
        "# 1. Hybrid KNN\n",
        "knn_output = knn_hybrid(X_test, X_train, y_train, test_ids, 11)\n",
        "y_pred_knn = knn_output[\"Hotness CLASS\"].tolist()\n",
        "\n",
        "acc_knn = evaluate_model(y_test_label, y_pred_knn)\n",
        "\n",
        "# 2. Gaussian Naive Bayes\n",
        "y_pred_gnb = train_gaussian_nb(X_train, y_train, X_test, threshold_low, threshold_high)\n",
        "acc_gnb = evaluate_model(y_test_label, y_pred_gnb)\n",
        "\n",
        "# 3. Gradient Boosting\n",
        "y_pred_gb, _ = train_gradient_boosting(X_train, y_train, X_test, threshold_low, threshold_high)\n",
        "acc_gb = evaluate_model(y_test_label, y_pred_gb)\n",
        "\n",
        "# Show table\n",
        "pd.DataFrame({\n",
        "    \"Model\": [\"GaussianNB\", \"Hybrid KNN\", \"Gradient Boosting\"],\n",
        "    \"Accuracy\": [acc_gnb, acc_knn, acc_gb]\n",
        "})\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cjsasihnpbB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}